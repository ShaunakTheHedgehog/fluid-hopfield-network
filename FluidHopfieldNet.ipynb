{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FluidHopfieldNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# In this file, I have implemented both the Modern Hopfield Network and the Fluid Hopfield Network. The first segment of \n",
        "# code below consists of helper functions, and the latter segments of code contain the actual implementations of the networks\n",
        "# themselves. Feel free to try running these networks on your own (or existing) semantic information datasets!"
      ],
      "metadata": {
        "id": "FByDVD9hj4Mi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we define some important helper functions for the Modern Hopfield Network and the Fluid Hopfield Network below!\n",
        "import numpy as np\n",
        "\n",
        "# softmax function takes in a vector 'x' and either a constant 'slope_param' or a vector 'slope_param' and applies the \n",
        "# softmax to 'x' with slope parameters given by 'slope_param'\n",
        "def softmax(x, slope_param):\n",
        "  y = (np.exp(x))**(slope_param)\n",
        "  tot = np.sum(y)\n",
        "  return y / tot\n",
        "\n",
        "# passes each element of a vector 'v' through sigmoid nonlinearity\n",
        "def vector_sigmoid(v):\n",
        "  denom = 1. + np.exp(-1 * v)\n",
        "  return np.ones(len(v)) / denom\n",
        "\n",
        "# applies sigmoid nonlinearity to the number 'x'\n",
        "def sigmoid(x):\n",
        "  return 1. / (1. + np.exp(-x))\n",
        "\n",
        "# detects whether 'v' is a \"sub-vector\" of 'w', i.e. all of the ones of 'v' are also ones of 'w'\n",
        "def isSubPattern(v, w):  \n",
        "  diff = w - v\n",
        "  return np.all(diff >= 0)\n",
        "\n",
        "# detects whether vector 'v' is a sub-pattern of any of the rows of the matrix 'matrix_of_vectors'\n",
        "def isSubPatternOfAny(v, matrix_of_vectors):\n",
        "  for w in matrix_of_vectors:\n",
        "    if isSubPattern(v, w):\n",
        "      return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "ytKdLyVFkcdm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we define the Modern Hopfield Network, which takes in 5 parameters, shown below. The matrix 'data' consists of \n",
        "# the feature vectors of the stored memories row-wise, and 'num_features' is the dimension of each of these rows. The \n",
        "# number of total stored memories is 'num_memories'. Finally, the slope parameter 'slope_param' and the time parameter\n",
        "# 'time_param' are specified. \n",
        "\n",
        "class ModernHopfieldNet:\n",
        "\n",
        "  # initialize a Modern Hopfield Network\n",
        "  def __init__(self, data, num_features, num_memories, slope_param, time_param):\n",
        "    (rows, cols) = np.shape(data)\n",
        "    assert num_features == cols\n",
        "    assert num_memories == rows\n",
        "    self.num_features = num_features\n",
        "    self.num_memories = num_memories\n",
        "    self.weights = data\n",
        "    self.slope_param = slope_param\n",
        "    self.time_param = time_param\n",
        "  \n",
        "  # pass feature pattern 'v' as input to network and perform 'num_epochs' cycles of updating 'v'\n",
        "  def forward(self, v, num_epochs):\n",
        "    v_curr = v \n",
        "    for t in range(num_epochs):\n",
        "      h = np.matmul(self.weights, v_curr)\n",
        "      weights_transpose = np.transpose(self.weights)\n",
        "      softmax_h = softmax(h, self.slope_param) \n",
        "      # update rule for network is implemented below\n",
        "      v_new = v_curr + (1.0 / self.time_param) * (np.matmul(weights_transpose, softmax_h) - v_curr)\n",
        "      v_curr = v_new\n",
        "\n",
        "    # for simplicity, we could round components of 'v_curr' to 0 or 1 to get a final binary vector as output:\n",
        "    # return np.around(v_curr)\n",
        "    # But, to be more rigorous, it would be better to actually use the 'self.__find_stored_mem' method, as described\n",
        "    # in the code for the Fluid Hopfield Net ('criterion' can be set to 0.1):\n",
        "    v_curr = self.__find_stored_mem(v_curr, 0.1)\n",
        "  \n",
        "  # this helper method records (Euclidean) distances of each of the stored memory vectors in the network to 'v_curr'\n",
        "  def __record_distances_to_mems(self, v_curr):\n",
        "    distances = np.zeros(self.weights.shape[0])\n",
        "    for i in range(len(self.weights)):\n",
        "      distances[i] = np.sqrt( np.sum( (self.weights[i] - v_curr)**2 ) )\n",
        "    return distances\n",
        "\n",
        "  # this helper function is used at the end of the 'forward' method. it takes in the vector 'v_curr' and a threshold\n",
        "  # value 'criterion' and returns closest stored memory vector that is within a Euclidean distance of 'criterion' from 'v_curr',\n",
        "  # if such a stored memory exists; else, it returns 'v_curr'  \n",
        "  def __find_stored_mem(self, v_curr, criterion):\n",
        "    distances = self.__record_distances_to_mems(v_curr)\n",
        "    indices = np.where(distances == np.min(distances))[0]\n",
        "    if (distances[indices[0]] < criterion):\n",
        "      return self.weights[indices[0]]\n",
        "    else:\n",
        "      return v_curr\n",
        "\n",
        "  # set slope parameter of network (i.e. the \"level of attention\" across all the stored memories)\n",
        "  def set_slope_param(self, slope_param):\n",
        "    self.slope_param = slope_param\n",
        "  \n",
        "  # set time parameter of network\n",
        "  def set_time_param(self, time_param):\n",
        "    self.time_param = time_param \n",
        "  \n",
        "  # returns slope parameter\n",
        "  def get_slope_param(self):\n",
        "    return self.slope_param \n",
        "  \n",
        "  # returns time parameter\n",
        "  def get_time_param(self):\n",
        "    return self.time_param"
      ],
      "metadata": {
        "id": "omZu9zO_kRLz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, the goal is to automate the process of giving attention to allow for the learning of novel concepts and robust \n",
        "# retrieval of existing memories. To achieve this, we incorporate a vector of slope parameters - one per memory - into\n",
        "# the network and have each slope parameter dynamically shift during each cycle of the 'forward' method.\n",
        "\n",
        "# below are hyperparameters of the Fluid Hopfield Network\n",
        "num_epochs = 20\n",
        "epsilon = 0.5\n",
        "decay = 0.1\n",
        "criterion = 0.1\n",
        "\n",
        "# We now define the Fluid Hopfield Network, which takes in 4 parameters, shown below. The matrix 'data' consists of \n",
        "# the feature vectors of the stored memories row-wise, and 'num_features' is the dimension of each of these rows. The \n",
        "# number of total stored memories is 'num_memories'. The time parameter 'time_param' is also specified. (Notice that no\n",
        "# slope parameters are specified to the network, since they are now automated!)\n",
        "\n",
        "class FluidHopfieldNet:\n",
        "\n",
        "  # initialize a Fluid Hopfield Network\n",
        "  def __init__(self, data, num_features, num_memories, time_param):\n",
        "    (rows, cols) = np.shape(data)\n",
        "    assert num_features == cols\n",
        "    assert num_memories == rows\n",
        "    self.num_features = num_features\n",
        "    self.num_memories = num_memories\n",
        "    self.weights = data\n",
        "    self.time_param = time_param\n",
        "    # initialize slope parameters for already-consolidated memories from 'data' to 1\n",
        "    self.theta = np.ones(rows)\n",
        "\n",
        "  # pass feature pattern 'v' as input to network and perform 'num_epochs' cycles of updating 'v'\n",
        "  def forward(self, v, num_epochs):\n",
        "\n",
        "    # check if 'v' is sub-pattern of any of the existing memories of network\n",
        "    if (not isSubPatternOfAny(v, self.weights)):\n",
        "      # if not, add it as a new memory to the network, with a low slope parameter of -1\n",
        "      self.theta = np.append(self.theta, -1.)\n",
        "      self.weights = np.append(self.weights, [v], axis=0)\n",
        "      self.num_memories += 1\n",
        "    \n",
        "    # for later analysis, one can store distance of current input pattern to each of the stored memories\n",
        "    distances_to_mems = np.zeros((num_epochs+1, self.num_memories))\n",
        "    distances_to_mems[0] = self.__record_distances_to_mems(v)\n",
        "\n",
        "    v_curr = v \n",
        "    for t in range(num_epochs):   \n",
        "      # compile the similiarities between 'v_curr' and each stored memory\n",
        "      h = np.matmul(self.weights, v_curr)\n",
        "      # pass slope parameters through sigmoid nonlinearity, and then apply them to each component of h\n",
        "      h = np.multiply(vector_sigmoid(self.theta), h)\n",
        "      weights_transpose = np.transpose(self.weights)\n",
        "      # scale the sigma(theta_j)'s by a factor of 10\n",
        "      softmax_h = softmax(h, 10.) \n",
        "      # same update rule as with Modern Hopfield Net, though 'softmax_h' is now the generalized softmax function\n",
        "      v_new = v_curr + (1.0 / self.time_param) * (np.matmul(weights_transpose, softmax_h) - v_curr)\n",
        "      v_curr = v_new\n",
        "      # dynamically update theta weights\n",
        "      self.__weight_update(v_curr)\n",
        "      # find distance of current input pattern to each of the stored memories\n",
        "      distances_to_mems[t+1] = self.__record_distances_to_mems(v_curr)\n",
        "\n",
        "    # if you'd like to analyze how distance of current input pattern to each of stored memories changes over the epochs\n",
        "    # you can uncomment the second part of the return statement below!\n",
        "    return self.__find_stored_mem(v_curr, criterion) #, distances_to_mems\n",
        "\n",
        "  # takes in current input to network 'v_curr' and updates the 'self.theta' based on similarity between 'v_curr' and \n",
        "  # each stored memory\n",
        "  def __weight_update(self, v_curr):\n",
        "    theta_new = np.zeros_like(self.theta)\n",
        "    weights_transpose = np.transpose(self.weights)\n",
        "    for j in range(len(self.weights)):\n",
        "      v = self.weights[j]\n",
        "      # using a cosine similarity metric between 'v_curr' and the stored memory 'v' -- it ranges from 0 to 1\n",
        "      similarity_rating = ( np.dot(v, v_curr) / (np.linalg.norm(v) * np.linalg.norm(v_curr)) )\n",
        "      # update rule for theta_j, the slope parameter for the jth stored memory \n",
        "      theta_new[j] = (self.theta[j] + (similarity_rating) * epsilon) - decay * np.absolute(self.theta[j])\n",
        "    self.theta = theta_new\n",
        "  \n",
        "  # this helper method records (Euclidean) distances of each of the stored memory vectors in the network to 'v_curr'\n",
        "  def __record_distances_to_mems(self, v_curr):\n",
        "    distances = np.zeros(self.weights.shape[0])\n",
        "    for i in range(len(self.weights)):\n",
        "      distances[i] = np.sqrt( np.sum( (self.weights[i] - v_curr)**2 ) )\n",
        "    return distances\n",
        "  \n",
        "  # this helper function is used at the end of the 'forward' method. it takes in the vector 'v_curr' and a threshold\n",
        "  # value 'criterion' and returns closest stored memory vector that is within a Euclidean distance of 'criterion' from 'v_curr',\n",
        "  # if such a stored memory exists; else, it returns 'v_curr'\n",
        "  def __find_stored_mem(self, v_curr, criterion):\n",
        "    distances = self.__record_distances_to_mems(v_curr)\n",
        "    indices = np.where(distances < criterion)[0]\n",
        "    if (len(indices) > 0):\n",
        "      return self.weights[indices[0]]\n",
        "    else:\n",
        "      return v_curr\n",
        "\n",
        "  # return the theta vector of slope parameters - one for each memory\n",
        "  def get_theta(self):\n",
        "    return self.theta\n",
        "  \n",
        "  # set time parameter of network to 'time_param'\n",
        "  def set_time_param(self, time_param):\n",
        "    self.time_param = time_param \n",
        "  \n",
        "  # returns time parameter\n",
        "  def get_time_param(self):\n",
        "    return self.time_param\n",
        "\n",
        "  # returns weights matrix, i.e. with the memory vectors of the network as its rows\n",
        "  def get_weights_matrix(self):\n",
        "    return self.weights"
      ],
      "metadata": {
        "id": "8ZavJCXCj50q"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}